{"cells":[{"cell_type":"markdown","metadata":{"id":"0yUOzXh0KEYf"},"source":["# Data-Mining Course (EECS 6412)\n","# Assignment (II): Decision Tree Classifier Implementation in Python\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"065A5OW9KOFA"},"source":["\n","## Objective: Implement a Decision Tree classifier in Python to gain a deeper understanding of its working principles.\n","\n","**Overal Instructions:**\n","\n","\n","*   Your task is to implement a Decision Tree classifier in Python.\n","*   The implementation has been broken down into multiple subfunctions, each with accompanying hints. Your goal is to complete the code for each function.\n","* You are only allowed to use the **pandas** and **numpy** libraries for this assignment. Some functions from Pandas have been provided for your convenience in the initial section, and you may use them if you feel they are necessary.\n","* Each part of your solution will be graded separately. However the sections are interrelated. It is crucial that your code is well-documented with comments explaining each part of your implementation.\n","* Please be aware that your responses will be thoroughly reviewed to ensure originality. Plagiarized or copied work will result in penalties.\n","\n","\n","**- Please skip the following descriptions and move directly to the Questions section if you are familiar with reading CSV files with Pandas library**"]},{"cell_type":"markdown","metadata":{"id":"pLSAGzhqGnNy"},"source":["\n","\n","---\n","\n","\n","##Please write your full name/names and student IDs here:\n","*   Full Name: Nadimul Hasan\n","*   Student ID: 216429516\n","\n","\n","*   Full Name: Jamie Fletcher\n","*   Student ID: 218287649"]},{"cell_type":"markdown","metadata":{"id":"UBJOIRZSOiDd"},"source":["\n","\n","---\n","\n","\n","\n","\n","\n","## Dataset Description for Car Acceptability Classification:\n"," Your codes must be general and should work on each tabular datasets with  categorical data types. For this example, have been provided with two datasets for training and testing- a training dataset (1400 samples) and a test dataset (327 samples). Pleased download datasets from [here](https://drive.google.com/drive/folders/1aka1ySucu1e3PqytQnVdEf63v9LT0E5z?usp=sharing). These samples represent the decisions of car experts regarding the acceptability of cars. The experts have categorized the cars into one of four classes: \"acceptable,\" \"unacceptable,\" \"good,\" or \"very good\" based on six categorical features.\n","\n","# Features:\n","\n","* **'BUYING':** This feature determines the purchase price of the car and is categorized into four classes: 'vhigh' (very high), 'high', 'med' (medium), or 'low'.\n","\n","* **'MAINTENANCE':** This feature indicates how high the car's maintenance cost is, and it is categorized into four classes: 'vhigh' (very high), 'high', 'med' (medium), or 'low'.\n","\n","* **'DOORS':** This featurte indicates number of the doors each car has: '2', '3', '4', '5more'(5 or more than 5 doors).\n","\n","* **'PERSONS':** This feature determines the car's capacity in terms of the number of persons it can accommodate and is categorized as '2', '4', or 'more'.\n","\n","* **'LUG_BOOT':** This feature represents the size of the car's luggage boot (trunk) and is categorized as 'small', 'med' (medium), or 'big'.\n","\n","* **'SAFETY':** This feature provides an estimate of the car's safety level and is categorized as 'low', 'med' (medium), or 'high'.\n","\n","* **'CLASS':** This is the target variable. It indicates the acceptance level of the car and is categorized as 'unacc' (unacceptable), 'acc' (acceptable), 'good', or 'vgood' (very good).\n","\n","**Please note that in this example the \"CLASS\" attribute is located at the last column of the tabular datasets**\n"]},{"cell_type":"markdown","metadata":{"id":"t8XuTyhMnwP5"},"source":["\n","\n","---\n","\n","\n","## Accessing the Datasets:\n","To access and read datasets from Google Drive in Google Colab using the Pandas library, you can follow these steps:\n","\n","1.   Upload CSV Files to Google Drive: First, ensure that you've uploaded the CSV files (train dataset and test dataset) to your Google Drive. You can create a folder for your project and upload the files there.\n","\n","\n","2.   Mount Google Drive in Google Colab:mount your Google Drive using the following code:\n"]},{"cell_type":"code","execution_count":131,"metadata":{},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'google.colab'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[1;32m/Users/nadimac/Documents/FALL 2023/EECS_4412/A2/A2_DT_CatOnly.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nadimac/Documents/FALL%202023/EECS_4412/A2/A2_DT_CatOnly.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m drive\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nadimac/Documents/FALL%202023/EECS_4412/A2/A2_DT_CatOnly.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m drive\u001b[39m.\u001b[39mmount(\u001b[39m'\u001b[39m\u001b[39m/content/drive\u001b[39m\u001b[39m'\u001b[39m)\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"T7tuWcl8zVkM"},"source":["\n","\n","3.   Access and Read Data using Pandas: You can access your CSV files in the mounted Google Drive directory. For example, if your CSV files are located in a folder named \"data-mining/assignment2/UG/\" in your Google Drive, you can read them as follows:\n","\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"w6jZmiO5zwrJ"},"outputs":[],"source":["import pandas as pd\n","\n","# Define the file paths for your CSV files\n","# train_csv_path = '/content/drive/MyDrive/data-mining/assignment2/UG/data_train_c.csv'\n","# test_csv_path = '/content/drive/MyDrive/data-mining/assignment2/UG/data_test_c.csv'\n","train_csv_path = 'data_train_c.csv'\n","test_csv_path = 'data_test_c.csv'\n","\n","# Read the data into Pandas DataFrames\n","train_df = pd.read_csv(train_csv_path)\n","test_df = pd.read_csv(test_csv_path)"]},{"cell_type":"markdown","metadata":{"id":"m34_HjXP2PZB"},"source":["\n","\n","4.   See Some Samples with head() Function:\n","\n","\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1695758736703,"user":{"displayName":"hamidreza dastmalchi","userId":"13779278108614878523"},"user_tz":240},"id":"abW4fCsV2U6O","outputId":"8aae6844-291d-4155-96bd-7ef9db12634e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Samples in the Training Dataset:\n","  BUYING MAINTENANCE  DOORS PERSONS LUG_BOOT SAFETY  CLASS\n","0  vhigh         low      2    more      med   high    acc\n","1  vhigh         low  5more       2      med   high  unacc\n","2    med       vhigh      2       2      med    low  unacc\n","3    med         low      2       2      med    low  unacc\n","4    low         med      2       2      big    low  unacc\n","1400\n","\n","Samples in the Test Dataset:\n","  BUYING MAINTENANCE  DOORS PERSONS LUG_BOOT SAFETY  CLASS\n","0    med       vhigh      2    more    small   high  unacc\n","1    low         low      4       4    small    med    acc\n","2    low         low      4    more    small    low  unacc\n","3  vhigh       vhigh      4       2      big    med  unacc\n","4  vhigh         med  5more       4    small   high    acc\n","327\n"]}],"source":["# See the first 5 samples in the training dataset\n","print(\"Samples in the Training Dataset:\")\n","print(train_df.head())\n","print(len(train_df))\n","\n","# See the first 5 samples in the test dataset\n","print(\"\\nSamples in the Test Dataset:\")\n","print(test_df.head())\n","print(len(test_df))"]},{"cell_type":"markdown","metadata":{"id":"9Xpw1xCD2jM9"},"source":["\n","\n","5.   Access Feature Names using columns Attribute:\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1695758736703,"user":{"displayName":"hamidreza dastmalchi","userId":"13779278108614878523"},"user_tz":240},"id":"LT9x4hgR2sRD","outputId":"2fd00c2b-4106-4ddd-d8bb-0976d30965b4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Feature Names:\n","Index(['BUYING', 'MAINTENANCE', 'DOORS', 'PERSONS', 'LUG_BOOT', 'SAFETY',\n","       'CLASS'],\n","      dtype='object')\n"]}],"source":["# Get the feature names (column names) of the training dataset\n","feature_names = train_df.columns\n","print(\"Feature Names:\")\n","print(feature_names)\n"]},{"cell_type":"markdown","metadata":{"id":"MfjUc4iy2wby"},"source":["6. Access Each Column as a Series:"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1695758736703,"user":{"displayName":"hamidreza dastmalchi","userId":"13779278108614878523"},"user_tz":240},"id":"FGWxsVWr248i","outputId":"f91fd98b-f93d-4783-a8b1-08b06b621677"},"outputs":[{"name":"stdout","output_type":"stream","text":["0    vhigh\n","1    vhigh\n","2      med\n","3      med\n","4      low\n","Name: BUYING, dtype: object\n","0      low\n","1      low\n","2    vhigh\n","3      low\n","4      med\n","Name: MAINTENANCE, dtype: object\n","0      acc\n","1    unacc\n","2    unacc\n","3    unacc\n","4    unacc\n","Name: CLASS, dtype: object\n","  BUYING  CLASS\n","0  vhigh    acc\n","1  vhigh  unacc\n","2    med  unacc\n","3    med  unacc\n","4    low  unacc\n","0     vhigh\n","10      low\n","15      low\n","19     high\n","Name: BUYING, dtype: object\n","['2' '5more' '3' '4']\n"]}],"source":["# Access the 'BUYING' column as a Series using square bracket notation\n","buying_price = train_df['BUYING']\n","print(buying_price.head())\n","\n","# Access the 'MAINTENANCE' column:\n","maintenance_cost = train_df['MAINTENANCE']\n","print(maintenance_cost.head())\n","\n","# Access the 'CLASS' column in the test dataset as a Series\n","labels = train_df['CLASS']\n","print(labels.head())\n","\n","temp = pd.DataFrame({\"BUYING\":buying_price,\"CLASS\": labels})\n","print(temp.head())\n","print((temp[temp[\"CLASS\"].isin([\"acc\", \"good\"])][\"BUYING\"]).head(4))\n","print(train_df[\"DOORS\"].unique())"]},{"cell_type":"markdown","metadata":{"id":"-JfDTOKe3sTm"},"source":["7. Use value_counts() function to  find the number of samples for each distinct value for a particular column:"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1695758736703,"user":{"displayName":"hamidreza dastmalchi","userId":"13779278108614878523"},"user_tz":240},"id":"QKLkuPXm46YG","outputId":"e00d7718-ce23-462a-9bb0-5e2fedf609d5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Counts of each distinct value in 'BUYING':\n","vhigh    355\n","high     355\n","low      350\n","med      340\n","Name: MAINTENANCE, dtype: int64\n","unacc    0.712143\n","acc      0.212857\n","good     0.039286\n","vgood    0.035714\n","Name: CLASS, dtype: float64\n","Index(['unacc', 'acc', 'good', 'vgood'], dtype='object')\n","0.039285714285714285\n","unacc\n","BUYING         vhigh\n","MAINTENANCE      low\n","DOORS              2\n","PERSONS         more\n","LUG_BOOT         med\n","SAFETY          high\n","CLASS            acc\n","Name: 0, dtype: object\n","vhigh\n"]}],"source":["print(\"Counts of each distinct value in 'BUYING':\")\n","print (maintenance_cost.value_counts())\n","freq_distr = labels.value_counts(normalize=True)\n","print(freq_distr)\n","print(freq_distr.index)\n","print(freq_distr[\"good\"])\n","print(labels.value_counts()[:1].index.tolist().pop())\n","print(train_df.loc[0])\n","heheh = {f'{index}': train_df.loc[0][index] for index in train_df.loc[0].index.tolist()}\n","print(heheh['BUYING'])"]},{"cell_type":"markdown","metadata":{"id":"ueIDC_tn_lLu"},"source":["---\n","---\n","\n","---\n","\n","\n","\n","\n","\n","\n","# Questions\n","---\n","\n","## - Part 1: Check Terminal Node Condition:\n","(Q.1., **5 Marks**): In the first step, we need to check if a node containing a DataFrame is a terminal node or it needs further splitting. Implement a function called \"check_if_terminal\" to do this task.\n","\n","Function Requirements:\n","\n","Input:\n","\n","\n","*   parent_data: the DataFrame corresponding to a node.\n","\n","*   threshold: Proportion threshold for the majority class.\n","\n","\n","\n","Calculate the proportion of samples with the majority class label.\n","\n","If the proportion â‰¥ threshold, return \"Leaf\" as flag.\n","\n","If the proportion < threshold, return \"Internal\" as the flag.\n","\n","In addition to the flag, the function must return majority class (\"acc\"/\"unacc\"/\"good\", \"vgood\")"]},{"cell_type":"code","execution_count":84,"metadata":{"id":"dkKqk351BFbr"},"outputs":[],"source":["from pandas import Series\n","\n","\n","def check_if_terminal(dataframe, threshold):\n","  # Get all attribute names from the DataFrame\n","  all_attrs = dataframe.columns\n","\n","  # Select the last attribute as the class attribute\n","  class_attrs: object = all_attrs[-1]\n","\n","  # Extract the labels (values of the class attribute)\n","  labels: object = dataframe[class_attrs]\n","\n","  #.................................\n","  # write the rest here:\n","\n","  # Find the freq. dist. of the unique values of the given column \n","  freq_dist: Series[int] = labels.value_counts(normalize = True)\n","\n","  # Find the value associated with the most freq. occurring value in the column\n","  majority_class = freq_dist[:1].index.to_list()[0]\n","\n","  flag = \"Internal\" if threshold > freq_dist.iloc[0] else \"Leaf\"\n","\n","  # output flag must be a string (whether \"Internal\" or \"Leaf\")\n","  # majority_class must be a string indicating the majority label of the samples in the node\n","  #..................................\n","  return flag, majority_class"]},{"cell_type":"code","execution_count":70,"metadata":{"id":"xyHOFah3-1Oh"},"outputs":[{"name":"stdout","output_type":"stream","text":["the node type is Internal\n","the majority class of the node is unacc\n"]}],"source":["# Check your implementation on training dataframe:\n","flag, majority_class = check_if_terminal(train_df, 0.9)\n","print(\"the node type is {}\".format(flag))\n","print(\"the majority class of the node is {}\".format(majority_class))"]},{"cell_type":"markdown","metadata":{"id":"vTq3CiKq5SFS"},"source":["\n","---\n","\n","## - Part 2: Entropy Function:\n","(Q.2.,  **10 Marks**): In order to split a node in a decision tree based on the Information Gain criterion, we need to calculate the entropy of the samples. Entropy is a measure of impurity in the data, and it is used to quantify the uncertainty associated with a set of class labels.\n","\n","\n","**Task:** Write a Python function called \"entropy\" that takes a the CLASS column of the dataframe denoted as \"label\" and returns the entropy as the output.\n","\n","Function Signature: def entropy(labels: list) -> float"]},{"cell_type":"code","execution_count":71,"metadata":{"id":"MkZoOwiZ7DMz"},"outputs":[],"source":["import numpy as np\n","\n","def entropy(labels):\n","  # Count the occurrences of each unique label\n","  value_counts: Series[int] = labels.value_counts()\n","  #.................................\n","  # write the rest here:\n","  rel_freq: Series[int] = labels.value_counts(normalize= True)\n","\n","  # Fetching the names of the different values of the column:\n","  indexes = value_counts.index\n","\n","  # Calculating entropy of the labels:\n","  entp = [(-1.0 * rel_freq[label]) * np.log2(rel_freq[label]) for label in indexes]\n","\n","  # Adding up the total entropy of the given column:\n","  entp = np.sum(entp)\n","\n","  #.................................\n","  #Return the calculated entropy\n","  return entp\n"]},{"cell_type":"code","execution_count":72,"metadata":{"id":"YAj5YHcz_bWG"},"outputs":[{"name":"stdout","output_type":"stream","text":["entropy of the node is 1.1790359988713874\n"]}],"source":["# Check your implementation on training dataframe:\n","labels = train_df[\"CLASS\"]\n","entrp = entropy(labels)\n","print(\"entropy of the node is {}\".format(entrp))"]},{"cell_type":"markdown","metadata":{"id":"sIUqZqOl8fie"},"source":["\n","\n","---\n","\n","## - Part 3: Calculating Information Gain:\n","(Q.3., **15 Marks**): In this step, you are required to implement a function named \"information_gain\" that computes the information gain obtained by splitting samples denoted by 'CLASS' column referenced as 'labels' based on a specific attribute column denoted as 'x'. It should be noted that both 'labels' and 'x' are columns of a DataFrame. Please use the function written in \"Part 2\" for this part.\n","\n"]},{"cell_type":"code","execution_count":73,"metadata":{"id":"izpdQGkh-PaM"},"outputs":[],"source":["def information_gain(x, labels):\n","  #Calculate the entropy of the parent node\n","  parent_entropy = entropy(labels)\n","  #.................................\n","  # write the rest here:\n","\n","  # Creating a new dataframe with the Target attr. column and the given column:\n","  df = pd.DataFrame({\"X\": x, \"CLASS\": labels})\n","\n","  # Getting the indexes and freq. of all the unique labels of the column:\n","  attr_indexes = x.value_counts().index\n","  freq = x.value_counts(normalize=True)\n","  x_ent = {}\n","  \n","  for label in attr_indexes:\n","    x_ent[label] = entropy(df[df[\"X\"].isin({label})][\"CLASS\"])\n","\n","  attr_entropy = 0.0\n","  for k, ent in x_ent.items():\n","    attr_entropy += freq[k] * ent\n","\n","  info_gain = parent_entropy - attr_entropy\n","\n","  #Calculate the information gain by subtracting child entropy from parent entropy\n","  #info_gain = parent_entropy - childs_entropy\n","  #.................................\n","  return info_gain"]},{"cell_type":"code","execution_count":74,"metadata":{"id":"_SbID8sz_uv4"},"outputs":[{"name":"stdout","output_type":"stream","text":["information gain of the node in splitting over PERSONS attribute is 0.21326310194104836\n"]}],"source":["# Check your implementation for training dataframe on \"PERSONS\" attribute:\n","labels = train_df[\"CLASS\"]\n","x = train_df['PERSONS']\n","info_gain = information_gain(x,labels)\n","print(\"information gain of the node in splitting over PERSONS attribute is {}\".format(info_gain))"]},{"cell_type":"markdown","metadata":{"id":"w_C2rfRQPNxE"},"source":["\n","\n","---\n","\n","\n","## - Part 4: Selecting the Best Attribute for Splitting\n","(Q.4., **10 Marks**): In this part, you are tasked with implementing a function called \"select_attribute.\" This function will take a parent DataFrame referenced as \"parent_data\" along with a list of splittable attributes denoted by \"remaining_attrs\" as the input and returns a string representing name of the best attribute which yields to the highest information gain after splitting. You may use the function written in \"Part 3\".\n"]},{"cell_type":"code","execution_count":75,"metadata":{"id":"PrRSD7ezQyyH"},"outputs":[],"source":["def select_attribute(parent_data, remaining_attrs):\n","  all_attrs = parent_data.columns\n","  # Extract the class attribute:\n","  class_attr = all_attrs[-1]\n","\n","  # Extract the labels (target values) from the parent data\n","  labels = parent_data[class_attr]\n","\n","  #.................................\n","  # write the rest here:\n","  # Loop through \"remaining_attrs\" attributes and calculate their information gains\n","  gains = {}\n","  for attr in remaining_attrs:\n","    gains[attr] = information_gain(parent_data[attr], labels)\n","\n","  attrs = list(gains.keys())\n","  attr_gains = list(gains.values())\n","\n","  sel_attr = attrs[attr_gains.index(max(attr_gains))]\n","  \n","  # Find the attribute with the highest information gain and return it as sel_attr\n","  #.................................\n","\n","  return sel_attr\n"]},{"cell_type":"code","execution_count":76,"metadata":{"id":"8SPm6-CnAgfh"},"outputs":[{"name":"stdout","output_type":"stream","text":["the best attribute for splitting the node is SAFETY\n"]}],"source":["# Check your implementation on training dataframe:\n","remaining_attrs = list(train_df.columns[:-1])\n","sel_attr = select_attribute(train_df, remaining_attrs)\n","print(\"the best attribute for splitting the node is {}\".format(sel_attr))"]},{"cell_type":"markdown","metadata":{"id":"1UH1AkKUrCI8"},"source":["\n","\n","\n","---\n","# - Part 5: Splitting the nodes at each tree level\n","\n","(Q.5., **20 Marks**):\n","\n","\n"," In this assignment, you will be implementing a crucial part of the decision tree implementation by creating a Python function called data_split. The purpose of this function is to split a parent node's dataframe into child dataframes based on the best attribute, which yields the highest information gain. You may use the helper functions that you have already implemented in previous sections.\n","\n","\n","**Instructions:**\n","* Write a function called \"data_split\" to split all the nodes in level \"n\" and to generate all the children nodes in level \"n+1\".\n","\n","* Perform node splitting in a systematic manner, progressing level by level. This entails creating all nodes at level n+1 by dividing all nodes eligible for splitting at level n. Refer to the example below for clarification:\n","\n","![Image](https://drive.google.com/uc?export=download&id=1kIOCkYaxUJMEKumBP6RxOLriQQY2Wlqx)\n","  As depicted in the illustration, at level 1, there is a solitary node designated as \"Node_1_1,\" symbolizing the first node of the first level. Level 1 has been subdivided into three nodes, identified as \"Node_2_1,\" \"Node_2_2,\" and \"Node_2_3,\" signifying the first, second, and third nodes of the second level of splitting. Please adhere to this notation for naming each node.\n","\n","* Imagine a dictionary named \"dataframe_dict,\" where the \"keys\" correspond to the node names at a specific splitting level, and the \"values\" represent the associated dataframes. To illustrate, for level 1, the \"dataframe_dict\" would consist of a single key, \"Node_1_1,\" with the corresponding value being the primary dataframe:\n","                dataframe_dict = {\"Node_1_1\": the main dataframe}\n","In this example, following the execution of the \"data_split\" function, the \"dataframe_dict\" dictionary should be replaced with a dictionary containing three entries, as demonstrated below:\n","      dataframe_dict = {\n","                            \"Node_2_1\": dataframe_2_1,\n","                            \"Node_2_2\": dataframe_2_2,\n","                            \"Node_2_3\": dataframe_2_3\n","                        }\n","\n","\n","\n","* Similarly, consider another dictionary called \"remaining_attrs\" with \"keys\" representing the nodes' names, and \"values\" representing the splittable attributes for each node.  For the first level, the \"remaining_attrs\" dictionary might be defined as:\n","\n","      remaining_attrs = {\n","                            \"Node_1_1\": ['BUYING', 'MAINTENANCE', 'DOORS', 'PERSONS', 'LUG_BOOT', 'SAFETY']\n","                        }\n","but after running the function \"data_split\", it would be updated to a dictionary with three keys-values as:\n","\n","      remaining_attrs = {\n","                         \"Node_2_1\": ['BUYING', 'MAINTENANCE', 'DOORS', 'LUG_BOOT', 'SAFETY'],\n","                         \"Node_2_2\": ['BUYING', 'MAINTENANCE', 'DOORS', 'LUG_BOOT', 'SAFETY'] ,\n","                         \"Node_2_3\": ['BUYING', 'MAINTENANCE', 'DOORS', 'LUG_BOOT', 'SAFETY']\n","                         }\n","\n","Please note that once we've performed a split on a categorical attribute such as \"PERSONS\" and generated the children nodes in the subsequent level, we are no longer permitted to split on the same categorical attribute within that branch of the tree. It's important to emphasize that this restriction doesn't apply to numerical attributes.\n","\n","In the context of this example, this means that the \"remaining_attrs\" dictionary is updated to a three-element dictionary, where none of the nodes in this specific branch have the \"PERSONS\" attribute as a splittable option anymore.\n","\n","* Consider the \"tree_model\" as a list containing three additional dictionaries: \"tree_connectivity\", \"node_labels\", \"and node_types\":\n","\n","            tree_model = [tree_connectivity , node_types, node_labels]\n","\n","where \"tree_connectivity\" is a dictionary representing the node connection to the parents. The \"node_types\" and \"node_labels\" are also dictionaries containing the (\"Leaf\" or \"Internal\") and the majority class for each node, respectively. Your \"data_split\" function must take the \"tree_model\" generated up to  level \"n\" and must update it to the model up top level \"n+1\" after splitting. See the below image for this example:\n","The tree_model at level 1 is:\n","\n","<img src=\"https://drive.google.com/uc?export=download&id=1GSJzh4CNE298LFXQR86LYpEfj4Q883-S\"  width=400>\n","\n","\n","After running the \"data_split\" function, the tree_model will be updated up to level 2 as follows:\n","\n","![Image](https://drive.google.com/uc?export=download&id=1Y3sGXHBpQtPVpMh8UnVlO0AYXvHNTGfo)\n","\n","\n","**Therefore**: You must write the function \"data_split\" which takes \"dataframe_dict\", \"remaining_attrs\", \"tree_model\", \"level\", and \"threshold\" as the input and update \"dataframe_dict\", \"remaining_attrs\", and \"tree_model\" upto level \"level+1\". The function must also retun a boolean flag \"stop_train\" which must be True if any child node is generated. Otherwise, it must return False. Here, input \"threshold\" is the majority class threshold for checking wether a node is a \"Leaf\" node or an \"Internal\" node.\n","\n","**To complete the function**:\n","* Loop through the nodes in \"dataframe_dict\" in the current \"level\". For each node, check if it's an \"Internal\" node and if so, find the best attribute for splitting. Create child nodes and finally update all the variables.\n","\n","\n","\n"]},{"cell_type":"code","execution_count":77,"metadata":{"id":"bAsxLUcTrFtV"},"outputs":[],"source":["def data_split(dataframe_dict, remaining_attrs, tree_model, level, threshold):\n","  # Unpack the tree_model list into three separate variables\n","  [tree_connectivity, node_labels, node_types] = tree_model\n","\n","  # Create an empty dictionary to store new child dataframes\n","  dataframe_dict_new = {}\n","  remaining_attrs_new = {}\n","\n","  # Initialize a counter for child nodes\n","  child_ind = 1\n","  #.................................\n","  # write the rest here:\n","  # Iterate over keys in the dataframe_dict (representing nodes at the current level)\n","\n","  new_node_labels = {}\n","  new_node_types = {}\n","\n","  stop_train: bool = True\n","\n","  for node_name, df in dataframe_dict.items():\n","    if node_types[node_name] != \"Leaf\" : #TODO: If node is leaf node, ignore [WIP]\n","      stop_train = False\n","\n","      # Updating the values of the children for the attributes remaining in the next level:\n","      cols_all_next_level = [attr for attr in remaining_attrs[node_name]]\n","\n","      # Finding the attribute to split on so that tree becomes for ex: Node-1-1 -> [Node-2-1, Node-2-2, Node-2-3]: \n","      col = select_attribute(df, remaining_attrs[node_name])\n","      cols_all_next_level.remove(col)\n","\n","      # Branches(children) we get from the parent node after splitting based on selected attribute:\n","      branches = df[col].unique()\n","\n","      connectivity = dict()\n","      # Iterating over the branches to create children:\n","      for branch in branches: \n","        \n","        # Node Name of the current child\n","        new_node_name = f'node_{level+1}_{child_ind}'\n","        remaining_attrs_new[new_node_name] = cols_all_next_level\n","\n","        # Update TREE connectivity of child:\n","        connectivity[f'{col}={branch}'] = new_node_name\n","\n","        # Picking examples(rows) where the value of the the attribute we're splitting on is equal to the value of the branch(child) it will be split into:\n","        branch_df = df[df[col] == branch]\n","\n","        # Adding the resulting DataFrame to the new datafram_dict\n","        dataframe_dict_new[new_node_name] = branch_df\n","\n","        node_type, majority_class = check_if_terminal(branch_df, threshold)\n","\n","        # Adding the values of the node labels of the child:\n","        new_node_labels[new_node_name] = majority_class\n","        new_node_types[new_node_name]= node_type\n","\n","        child_ind += 1\n","      \n","      tree_connectivity[node_name] = connectivity\n","\n","\n","  # Replace the new old dictionaries with new dictionaries:\n","  dataframe_dict = dataframe_dict_new\n","  remaining_attrs = remaining_attrs_new\n","\n","  # Update the tree_model and return it:\n","  node_labels.update(new_node_labels)\n","  node_types.update(new_node_types)\n","  tree_model = [tree_connectivity, node_labels, node_types]\n","\n","  # also return True as stop_train if no child node is generated. Otherwise return False\n","#.................................\n","  # Return the updated tree_model and a flag indicating whether training should stop\n","  return tree_model, stop_train, dataframe_dict, remaining_attrs\n"]},{"cell_type":"code","execution_count":78,"metadata":{"id":"TbJDC_sFA5jm"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n"," tree connectivity:\n","{'node_1_1': {'SAFETY=high': 'node_2_1', 'SAFETY=low': 'node_2_2', 'SAFETY=med': 'node_2_3'}}\n","\n"," node labels:\n","{'node_1_1': 'unacc', 'node_2_1': 'unacc', 'node_2_2': 'unacc', 'node_2_3': 'unacc'}\n","\n"," node types:\n","{'node_1_1': 'Internal', 'node_2_1': 'Internal', 'node_2_2': 'Leaf', 'node_2_3': 'Internal'}\n","\n"," remaining attributes are:\n","{'node_2_1': ['BUYING', 'MAINTENANCE', 'DOORS', 'PERSONS', 'LUG_BOOT'], 'node_2_2': ['BUYING', 'MAINTENANCE', 'DOORS', 'PERSONS', 'LUG_BOOT'], 'node_2_3': ['BUYING', 'MAINTENANCE', 'DOORS', 'PERSONS', 'LUG_BOOT']}\n"]}],"source":["# Now Check your implementation on training dataframe:\n","# Initializing\n","threshold = 0.9\n","\n","tree_connectivity = {}\n","\n","flag, majority_class = check_if_terminal(train_df, 0.9)\n","\n","node_types = {\"node_1_1\": flag}\n","node_labels = {\"node_1_1\": majority_class}\n","\n","# Create an initial tree_model\n","tree_model = [tree_connectivity, node_labels, node_types]\n","\n","# Create an initial dataframe_dict\n","dataframe_dict = {\"node_1_1\": train_df}\n","\n","# Create an initial remaining_attrs\n","\n","independent_attrs = list(train_df.columns[:-1])\n","remaining_attrs = {\"node_1_1\": independent_attrs}\n","\n","# Set level to 1\n","level = 1\n","\n","\n","# Update tree model\n","tree_model, stop_train, dataframe_dict, remaining_attrs = data_split(dataframe_dict, remaining_attrs, tree_model, level, threshold)\n","[tree_connectivity, node_labels, node_types] = tree_model\n","print(\"\\n tree connectivity:\")\n","print(tree_connectivity)\n","\n","print(\"\\n node labels:\")\n","print(node_labels)\n","\n","print(\"\\n node types:\")\n","print(node_types)\n","\n","print(\"\\n remaining attributes are:\")\n","print(remaining_attrs)\n"]},{"cell_type":"markdown","metadata":{"id":"-K2ROrMYyUfR"},"source":["\n","\n","---\n","\n","## -Part 6: Training the Decision Tree\n","\n","(Q.6., **10 Marks**): Now, let's create a function called \"tree_train\" to train the decision tree. This function begins by initializing the tree model and dataframe dictionary using the root node named \"node_1_1.\" It then iteratively updates these structures as it progresses through the tree, continuing until no further child nodes are generated. The process starts at level 1, and with each iteration, the level is incremented. Importantly, make sure to utilize the \"split_data\" function, which you've previously implemented, to assist in the tree construction. Ultimately, the function must return the fully trained tree model.\n","\n"]},{"cell_type":"code","execution_count":79,"metadata":{"id":"1EVuxiv2zD8c"},"outputs":[],"source":["def tree_train(training_data, threshold):\n","  # Initializing\n","  tree_connectivity = {}\n","\n","  flag, majority_class = check_if_terminal(training_data, threshold)\n","\n","  node_types = {\"node_1_1\": flag}\n","  node_labels = {\"node_1_1\": majority_class}\n","\n","  # Create a tree_model list to store connectivity, node labels, and node types\n","  tree_model = [tree_connectivity, node_labels, node_types]\n","\n","  # Create a dataframe_dict with the initial training data and associate it with the root node\n","  dataframe_dict = {\"node_1_1\": training_data}\n","\n","  # Create a remaining_attrs dictionary with all the independent attributes and associate it with the root node\n","  indp_attrs = list(training_data.columns[:-1])\n","  remaining_attrs = {\"node_1_1\": indp_attrs}\n","\n","  # Initialize the level of the tree to 1\n","  level = 1\n","\n","\n","  # Continue tree construction until a stopping condition is met (use while loop)\n","    #.................................\n","  # write the rest here:\n","  # write a loop function and exit the loop if terminating criterion is met\n","  control = False\n","\n","  while (not control):\n","    tree_model, stop_train, dataframe_dict, remaining_attrs = data_split(dataframe_dict, remaining_attrs, tree_model, level, threshold)\n","    level += 1 \n","    control = stop_train\n","    \n","  #.................................\n","  # Return the final tree model\n","  return tree_model"]},{"cell_type":"code","execution_count":80,"metadata":{"id":"cHS5ytxLDEdc"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n"," tree connectivity:\n","{'node_1_1': {'SAFETY=high': 'node_2_1', 'SAFETY=low': 'node_2_2', 'SAFETY=med': 'node_2_3'}, 'node_2_1': {'PERSONS=more': 'node_3_1', 'PERSONS=2': 'node_3_2', 'PERSONS=4': 'node_3_3'}, 'node_2_3': {'PERSONS=4': 'node_3_4', 'PERSONS=more': 'node_3_5', 'PERSONS=2': 'node_3_6'}, 'node_3_1': {'BUYING=vhigh': 'node_4_1', 'BUYING=low': 'node_4_2', 'BUYING=high': 'node_4_3', 'BUYING=med': 'node_4_4'}, 'node_3_3': {'BUYING=high': 'node_4_5', 'BUYING=low': 'node_4_6', 'BUYING=vhigh': 'node_4_7', 'BUYING=med': 'node_4_8'}, 'node_3_4': {'BUYING=vhigh': 'node_4_9', 'BUYING=low': 'node_4_10', 'BUYING=high': 'node_4_11', 'BUYING=med': 'node_4_12'}, 'node_3_5': {'BUYING=high': 'node_4_13', 'BUYING=low': 'node_4_14', 'BUYING=med': 'node_4_15', 'BUYING=vhigh': 'node_4_16'}, 'node_4_1': {'MAINTENANCE=low': 'node_5_1', 'MAINTENANCE=high': 'node_5_2', 'MAINTENANCE=vhigh': 'node_5_3', 'MAINTENANCE=med': 'node_5_4'}, 'node_4_2': {'MAINTENANCE=low': 'node_5_5', 'MAINTENANCE=high': 'node_5_6', 'MAINTENANCE=med': 'node_5_7', 'MAINTENANCE=vhigh': 'node_5_8'}, 'node_4_3': {'MAINTENANCE=vhigh': 'node_5_9', 'MAINTENANCE=high': 'node_5_10', 'MAINTENANCE=low': 'node_5_11', 'MAINTENANCE=med': 'node_5_12'}, 'node_4_4': {'MAINTENANCE=vhigh': 'node_5_13', 'MAINTENANCE=med': 'node_5_14', 'MAINTENANCE=high': 'node_5_15', 'MAINTENANCE=low': 'node_5_16'}, 'node_4_5': {'MAINTENANCE=low': 'node_5_17', 'MAINTENANCE=high': 'node_5_18', 'MAINTENANCE=vhigh': 'node_5_19', 'MAINTENANCE=med': 'node_5_20'}, 'node_4_6': {'MAINTENANCE=med': 'node_5_21', 'MAINTENANCE=high': 'node_5_22', 'MAINTENANCE=low': 'node_5_23', 'MAINTENANCE=vhigh': 'node_5_24'}, 'node_4_7': {'MAINTENANCE=low': 'node_5_25', 'MAINTENANCE=high': 'node_5_26', 'MAINTENANCE=med': 'node_5_27', 'MAINTENANCE=vhigh': 'node_5_28'}, 'node_4_8': {'MAINTENANCE=high': 'node_5_29', 'MAINTENANCE=med': 'node_5_30', 'MAINTENANCE=low': 'node_5_31', 'MAINTENANCE=vhigh': 'node_5_32'}, 'node_4_9': {'MAINTENANCE=med': 'node_5_33', 'MAINTENANCE=vhigh': 'node_5_34', 'MAINTENANCE=low': 'node_5_35', 'MAINTENANCE=high': 'node_5_36'}, 'node_4_10': {'MAINTENANCE=vhigh': 'node_5_37', 'MAINTENANCE=high': 'node_5_38', 'MAINTENANCE=low': 'node_5_39', 'MAINTENANCE=med': 'node_5_40'}, 'node_4_11': {'LUG_BOOT=small': 'node_5_41', 'LUG_BOOT=med': 'node_5_42', 'LUG_BOOT=big': 'node_5_43'}, 'node_4_12': {'MAINTENANCE=low': 'node_5_44', 'MAINTENANCE=vhigh': 'node_5_45', 'MAINTENANCE=med': 'node_5_46', 'MAINTENANCE=high': 'node_5_47'}, 'node_4_13': {'LUG_BOOT=med': 'node_5_48', 'LUG_BOOT=small': 'node_5_49', 'LUG_BOOT=big': 'node_5_50'}, 'node_4_14': {'MAINTENANCE=vhigh': 'node_5_51', 'MAINTENANCE=high': 'node_5_52', 'MAINTENANCE=low': 'node_5_53', 'MAINTENANCE=med': 'node_5_54'}, 'node_4_15': {'MAINTENANCE=low': 'node_5_55', 'MAINTENANCE=vhigh': 'node_5_56', 'MAINTENANCE=med': 'node_5_57', 'MAINTENANCE=high': 'node_5_58'}, 'node_4_16': {'MAINTENANCE=low': 'node_5_59', 'MAINTENANCE=vhigh': 'node_5_60', 'MAINTENANCE=high': 'node_5_61', 'MAINTENANCE=med': 'node_5_62'}, 'node_5_4': {'DOORS=3': 'node_6_1', 'DOORS=2': 'node_6_2', 'DOORS=4': 'node_6_3', 'DOORS=5more': 'node_6_4'}, 'node_5_5': {'LUG_BOOT=big': 'node_6_5', 'LUG_BOOT=med': 'node_6_6', 'LUG_BOOT=small': 'node_6_7'}, 'node_5_6': {'LUG_BOOT=small': 'node_6_8', 'LUG_BOOT=big': 'node_6_9', 'LUG_BOOT=med': 'node_6_10'}, 'node_5_7': {'LUG_BOOT=small': 'node_6_11', 'LUG_BOOT=big': 'node_6_12', 'LUG_BOOT=med': 'node_6_13'}, 'node_5_8': {'DOORS=4': 'node_6_14', 'DOORS=3': 'node_6_15', 'DOORS=2': 'node_6_16', 'DOORS=5more': 'node_6_17'}, 'node_5_10': {'DOORS=4': 'node_6_18', 'DOORS=2': 'node_6_19', 'DOORS=5more': 'node_6_20', 'DOORS=3': 'node_6_21'}, 'node_5_14': {'LUG_BOOT=small': 'node_6_22', 'LUG_BOOT=med': 'node_6_23', 'LUG_BOOT=big': 'node_6_24'}, 'node_5_15': {'DOORS=5more': 'node_6_25', 'DOORS=3': 'node_6_26', 'DOORS=4': 'node_6_27', 'DOORS=2': 'node_6_28'}, 'node_5_16': {'LUG_BOOT=med': 'node_6_29', 'LUG_BOOT=big': 'node_6_30', 'LUG_BOOT=small': 'node_6_31'}, 'node_5_21': {'LUG_BOOT=big': 'node_6_32', 'LUG_BOOT=small': 'node_6_33', 'LUG_BOOT=med': 'node_6_34'}, 'node_5_22': {'LUG_BOOT=big': 'node_6_35', 'LUG_BOOT=small': 'node_6_36', 'LUG_BOOT=med': 'node_6_37'}, 'node_5_23': {'LUG_BOOT=big': 'node_6_38', 'LUG_BOOT=small': 'node_6_39', 'LUG_BOOT=med': 'node_6_40'}, 'node_5_30': {'LUG_BOOT=med': 'node_6_41', 'LUG_BOOT=big': 'node_6_42', 'LUG_BOOT=small': 'node_6_43'}, 'node_5_31': {'LUG_BOOT=med': 'node_6_44', 'LUG_BOOT=small': 'node_6_45', 'LUG_BOOT=big': 'node_6_46'}, 'node_5_33': {'LUG_BOOT=med': 'node_6_47', 'LUG_BOOT=small': 'node_6_48', 'LUG_BOOT=big': 'node_6_49'}, 'node_5_35': {'LUG_BOOT=med': 'node_6_50', 'LUG_BOOT=small': 'node_6_51', 'LUG_BOOT=big': 'node_6_52'}, 'node_5_37': {'LUG_BOOT=big': 'node_6_53', 'LUG_BOOT=med': 'node_6_54', 'LUG_BOOT=small': 'node_6_55'}, 'node_5_39': {'LUG_BOOT=med': 'node_6_56', 'LUG_BOOT=big': 'node_6_57', 'LUG_BOOT=small': 'node_6_58'}, 'node_5_40': {'LUG_BOOT=med': 'node_6_59', 'LUG_BOOT=small': 'node_6_60', 'LUG_BOOT=big': 'node_6_61'}, 'node_5_42': {'DOORS=5more': 'node_6_62', 'DOORS=3': 'node_6_63', 'DOORS=2': 'node_6_64', 'DOORS=4': 'node_6_65'}, 'node_5_43': {'MAINTENANCE=low': 'node_6_66', 'MAINTENANCE=vhigh': 'node_6_67', 'MAINTENANCE=med': 'node_6_68', 'MAINTENANCE=high': 'node_6_69'}, 'node_5_44': {'LUG_BOOT=big': 'node_6_70', 'LUG_BOOT=med': 'node_6_71', 'LUG_BOOT=small': 'node_6_72'}, 'node_5_45': {'LUG_BOOT=med': 'node_6_73', 'LUG_BOOT=small': 'node_6_74', 'LUG_BOOT=big': 'node_6_75'}, 'node_5_47': {'LUG_BOOT=small': 'node_6_76', 'LUG_BOOT=big': 'node_6_77', 'LUG_BOOT=med': 'node_6_78'}, 'node_5_48': {'MAINTENANCE=med': 'node_6_79', 'MAINTENANCE=high': 'node_6_80', 'MAINTENANCE=low': 'node_6_81', 'MAINTENANCE=vhigh': 'node_6_82'}, 'node_5_50': {'MAINTENANCE=vhigh': 'node_6_83', 'MAINTENANCE=med': 'node_6_84', 'MAINTENANCE=high': 'node_6_85', 'MAINTENANCE=low': 'node_6_86'}, 'node_5_51': {'LUG_BOOT=med': 'node_6_87', 'LUG_BOOT=big': 'node_6_88', 'LUG_BOOT=small': 'node_6_89'}, 'node_5_53': {'LUG_BOOT=big': 'node_6_90', 'LUG_BOOT=small': 'node_6_91', 'LUG_BOOT=med': 'node_6_92'}, 'node_5_54': {'LUG_BOOT=big': 'node_6_93', 'LUG_BOOT=med': 'node_6_94', 'LUG_BOOT=small': 'node_6_95'}, 'node_5_55': {'LUG_BOOT=big': 'node_6_96', 'LUG_BOOT=med': 'node_6_97', 'LUG_BOOT=small': 'node_6_98'}, 'node_5_56': {'LUG_BOOT=big': 'node_6_99', 'LUG_BOOT=med': 'node_6_100', 'LUG_BOOT=small': 'node_6_101'}, 'node_5_58': {'LUG_BOOT=big': 'node_6_102', 'LUG_BOOT=small': 'node_6_103', 'LUG_BOOT=med': 'node_6_104'}, 'node_5_59': {'LUG_BOOT=small': 'node_6_105', 'LUG_BOOT=med': 'node_6_106', 'LUG_BOOT=big': 'node_6_107'}, 'node_5_62': {'LUG_BOOT=small': 'node_6_108', 'LUG_BOOT=med': 'node_6_109', 'LUG_BOOT=big': 'node_6_110'}, 'node_6_2': {'LUG_BOOT=med': 'node_7_1', 'LUG_BOOT=small': 'node_7_2', 'LUG_BOOT=big': 'node_7_3'}, 'node_6_6': {'DOORS=4': 'node_7_4', 'DOORS=2': 'node_7_5', 'DOORS=3': 'node_7_6', 'DOORS=5more': 'node_7_7'}, 'node_6_8': {'DOORS=3': 'node_7_8', 'DOORS=4': 'node_7_9', 'DOORS=5more': 'node_7_10', 'DOORS=2': 'node_7_11'}, 'node_6_11': {'DOORS=3': 'node_7_12', 'DOORS=2': 'node_7_13', 'DOORS=4': 'node_7_14'}, 'node_6_16': {'LUG_BOOT=med': 'node_7_15', 'LUG_BOOT=small': 'node_7_16'}, 'node_6_19': {'LUG_BOOT=small': 'node_7_17', 'LUG_BOOT=big': 'node_7_18'}, 'node_6_22': {'DOORS=5more': 'node_7_19', 'DOORS=4': 'node_7_20', 'DOORS=2': 'node_7_21', 'DOORS=3': 'node_7_22'}, 'node_6_23': {'DOORS=3': 'node_7_23', 'DOORS=5more': 'node_7_24', 'DOORS=4': 'node_7_25', 'DOORS=2': 'node_7_26'}, 'node_6_28': {'LUG_BOOT=small': 'node_7_27', 'LUG_BOOT=med': 'node_7_28'}, 'node_6_29': {'DOORS=2': 'node_7_29', 'DOORS=4': 'node_7_30', 'DOORS=3': 'node_7_31'}, 'node_6_31': {'DOORS=2': 'node_7_32', 'DOORS=4': 'node_7_33', 'DOORS=3': 'node_7_34', 'DOORS=5more': 'node_7_35'}, 'node_6_34': {'DOORS=4': 'node_7_36', 'DOORS=2': 'node_7_37', 'DOORS=5more': 'node_7_38', 'DOORS=3': 'node_7_39'}, 'node_6_37': {'DOORS=2': 'node_7_40', 'DOORS=4': 'node_7_41', 'DOORS=5more': 'node_7_42'}, 'node_6_40': {'DOORS=4': 'node_7_43', 'DOORS=3': 'node_7_44', 'DOORS=2': 'node_7_45'}, 'node_6_41': {'DOORS=5more': 'node_7_46', 'DOORS=4': 'node_7_47', 'DOORS=2': 'node_7_48', 'DOORS=3': 'node_7_49'}, 'node_6_44': {'DOORS=4': 'node_7_50', 'DOORS=2': 'node_7_51', 'DOORS=5more': 'node_7_52'}, 'node_6_47': {'DOORS=3': 'node_7_53', 'DOORS=4': 'node_7_54', 'DOORS=5more': 'node_7_55'}, 'node_6_50': {'DOORS=4': 'node_7_56', 'DOORS=2': 'node_7_57', 'DOORS=5more': 'node_7_58', 'DOORS=3': 'node_7_59'}, 'node_6_54': {'DOORS=4': 'node_7_60', 'DOORS=2': 'node_7_61', 'DOORS=5more': 'node_7_62'}, 'node_6_56': {'DOORS=2': 'node_7_63', 'DOORS=4': 'node_7_64', 'DOORS=5more': 'node_7_65', 'DOORS=3': 'node_7_66'}, 'node_6_59': {'DOORS=3': 'node_7_67', 'DOORS=2': 'node_7_68', 'DOORS=5more': 'node_7_69'}, 'node_6_62': {'MAINTENANCE=med': 'node_7_70', 'MAINTENANCE=low': 'node_7_71', 'MAINTENANCE=vhigh': 'node_7_72', 'MAINTENANCE=high': 'node_7_73'}, 'node_6_65': {'MAINTENANCE=low': 'node_7_74', 'MAINTENANCE=vhigh': 'node_7_75', 'MAINTENANCE=high': 'node_7_76', 'MAINTENANCE=med': 'node_7_77'}, 'node_6_71': {'DOORS=2': 'node_7_78', 'DOORS=3': 'node_7_79', 'DOORS=5more': 'node_7_80', 'DOORS=4': 'node_7_81'}, 'node_6_73': {'DOORS=2': 'node_7_82', 'DOORS=4': 'node_7_83', 'DOORS=5more': 'node_7_84', 'DOORS=3': 'node_7_85'}, 'node_6_79': {'DOORS=2': 'node_7_86', 'DOORS=3': 'node_7_87', 'DOORS=5more': 'node_7_88', 'DOORS=4': 'node_7_89'}, 'node_6_80': {'DOORS=2': 'node_7_90', 'DOORS=3': 'node_7_91', 'DOORS=5more': 'node_7_92', 'DOORS=4': 'node_7_93'}, 'node_6_87': {'DOORS=2': 'node_7_94', 'DOORS=5more': 'node_7_95', 'DOORS=4': 'node_7_96', 'DOORS=3': 'node_7_97'}, 'node_6_94': {'DOORS=5more': 'node_7_98', 'DOORS=2': 'node_7_99', 'DOORS=4': 'node_7_100'}, 'node_6_95': {'DOORS=2': 'node_7_101', 'DOORS=5more': 'node_7_102'}, 'node_6_98': {'DOORS=3': 'node_7_103', 'DOORS=5more': 'node_7_104', 'DOORS=2': 'node_7_105'}, 'node_6_100': {'DOORS=2': 'node_7_106', 'DOORS=3': 'node_7_107', 'DOORS=5more': 'node_7_108', 'DOORS=4': 'node_7_109'}, 'node_6_104': {'DOORS=2': 'node_7_110', 'DOORS=5more': 'node_7_111'}, 'node_6_106': {'DOORS=2': 'node_7_112', 'DOORS=4': 'node_7_113', 'DOORS=3': 'node_7_114'}, 'node_6_109': {'DOORS=5more': 'node_7_115', 'DOORS=2': 'node_7_116', 'DOORS=4': 'node_7_117', 'DOORS=3': 'node_7_118'}}\n","\n"," node labels:\n","{'node_1_1': 'unacc', 'node_2_1': 'unacc', 'node_2_2': 'unacc', 'node_2_3': 'unacc', 'node_3_1': 'acc', 'node_3_2': 'unacc', 'node_3_3': 'acc', 'node_3_4': 'unacc', 'node_3_5': 'acc', 'node_3_6': 'unacc', 'node_4_1': 'unacc', 'node_4_2': 'vgood', 'node_4_3': 'acc', 'node_4_4': 'acc', 'node_4_5': 'acc', 'node_4_6': 'vgood', 'node_4_7': 'acc', 'node_4_8': 'acc', 'node_4_9': 'unacc', 'node_4_10': 'acc', 'node_4_11': 'unacc', 'node_4_12': 'acc', 'node_4_13': 'unacc', 'node_4_14': 'acc', 'node_4_15': 'acc', 'node_4_16': 'unacc', 'node_5_1': 'acc', 'node_5_2': 'unacc', 'node_5_3': 'unacc', 'node_5_4': 'acc', 'node_5_5': 'vgood', 'node_5_6': 'vgood', 'node_5_7': 'vgood', 'node_5_8': 'acc', 'node_5_9': 'unacc', 'node_5_10': 'acc', 'node_5_11': 'acc', 'node_5_12': 'acc', 'node_5_13': 'acc', 'node_5_14': 'vgood', 'node_5_15': 'acc', 'node_5_16': 'vgood', 'node_5_17': 'acc', 'node_5_18': 'acc', 'node_5_19': 'unacc', 'node_5_20': 'acc', 'node_5_21': 'vgood', 'node_5_22': 'vgood', 'node_5_23': 'good', 'node_5_24': 'acc', 'node_5_25': 'acc', 'node_5_26': 'unacc', 'node_5_27': 'acc', 'node_5_28': 'unacc', 'node_5_29': 'acc', 'node_5_30': 'acc', 'node_5_31': 'vgood', 'node_5_32': 'acc', 'node_5_33': 'unacc', 'node_5_34': 'unacc', 'node_5_35': 'unacc', 'node_5_36': 'unacc', 'node_5_37': 'acc', 'node_5_38': 'acc', 'node_5_39': 'good', 'node_5_40': 'acc', 'node_5_41': 'unacc', 'node_5_42': 'unacc', 'node_5_43': 'acc', 'node_5_44': 'good', 'node_5_45': 'unacc', 'node_5_46': 'acc', 'node_5_47': 'acc', 'node_5_48': 'acc', 'node_5_49': 'unacc', 'node_5_50': 'acc', 'node_5_51': 'acc', 'node_5_52': 'acc', 'node_5_53': 'good', 'node_5_54': 'good', 'node_5_55': 'good', 'node_5_56': 'acc', 'node_5_57': 'acc', 'node_5_58': 'acc', 'node_5_59': 'unacc', 'node_5_60': 'unacc', 'node_5_61': 'unacc', 'node_5_62': 'acc', 'node_6_1': 'acc', 'node_6_2': 'acc', 'node_6_3': 'acc', 'node_6_4': 'acc', 'node_6_5': 'vgood', 'node_6_6': 'vgood', 'node_6_7': 'good', 'node_6_8': 'acc', 'node_6_9': 'vgood', 'node_6_10': 'vgood', 'node_6_11': 'good', 'node_6_12': 'vgood', 'node_6_13': 'vgood', 'node_6_14': 'acc', 'node_6_15': 'acc', 'node_6_16': 'acc', 'node_6_17': 'acc', 'node_6_18': 'acc', 'node_6_19': 'unacc', 'node_6_20': 'acc', 'node_6_21': 'acc', 'node_6_22': 'acc', 'node_6_23': 'vgood', 'node_6_24': 'vgood', 'node_6_25': 'acc', 'node_6_26': 'acc', 'node_6_27': 'acc', 'node_6_28': 'unacc', 'node_6_29': 'vgood', 'node_6_30': 'vgood', 'node_6_31': 'good', 'node_6_32': 'vgood', 'node_6_33': 'good', 'node_6_34': 'vgood', 'node_6_35': 'vgood', 'node_6_36': 'acc', 'node_6_37': 'vgood', 'node_6_38': 'vgood', 'node_6_39': 'good', 'node_6_40': 'good', 'node_6_41': 'vgood', 'node_6_42': 'vgood', 'node_6_43': 'acc', 'node_6_44': 'vgood', 'node_6_45': 'good', 'node_6_46': 'vgood', 'node_6_47': 'acc', 'node_6_48': 'unacc', 'node_6_49': 'acc', 'node_6_50': 'acc', 'node_6_51': 'unacc', 'node_6_52': 'acc', 'node_6_53': 'acc', 'node_6_54': 'acc', 'node_6_55': 'unacc', 'node_6_56': 'acc', 'node_6_57': 'good', 'node_6_58': 'acc', 'node_6_59': 'acc', 'node_6_60': 'acc', 'node_6_61': 'good', 'node_6_62': 'acc', 'node_6_63': 'unacc', 'node_6_64': 'unacc', 'node_6_65': 'acc', 'node_6_66': 'acc', 'node_6_67': 'unacc', 'node_6_68': 'acc', 'node_6_69': 'acc', 'node_6_70': 'good', 'node_6_71': 'acc', 'node_6_72': 'acc', 'node_6_73': 'unacc', 'node_6_74': 'unacc', 'node_6_75': 'acc', 'node_6_76': 'unacc', 'node_6_77': 'acc', 'node_6_78': 'acc', 'node_6_79': 'acc', 'node_6_80': 'acc', 'node_6_81': 'acc', 'node_6_82': 'unacc', 'node_6_83': 'unacc', 'node_6_84': 'acc', 'node_6_85': 'acc', 'node_6_86': 'acc', 'node_6_87': 'acc', 'node_6_88': 'acc', 'node_6_89': 'unacc', 'node_6_90': 'good', 'node_6_91': 'acc', 'node_6_92': 'good', 'node_6_93': 'good', 'node_6_94': 'good', 'node_6_95': 'unacc', 'node_6_96': 'good', 'node_6_97': 'good', 'node_6_98': 'acc', 'node_6_99': 'acc', 'node_6_100': 'acc', 'node_6_101': 'unacc', 'node_6_102': 'acc', 'node_6_103': 'unacc', 'node_6_104': 'unacc', 'node_6_105': 'unacc', 'node_6_106': 'acc', 'node_6_107': 'acc', 'node_6_108': 'unacc', 'node_6_109': 'acc', 'node_6_110': 'acc', 'node_7_1': 'acc', 'node_7_2': 'unacc', 'node_7_3': 'acc', 'node_7_4': 'vgood', 'node_7_5': 'good', 'node_7_6': 'vgood', 'node_7_7': 'vgood', 'node_7_8': 'acc', 'node_7_9': 'acc', 'node_7_10': 'acc', 'node_7_11': 'unacc', 'node_7_12': 'good', 'node_7_13': 'unacc', 'node_7_14': 'good', 'node_7_15': 'acc', 'node_7_16': 'unacc', 'node_7_17': 'unacc', 'node_7_18': 'acc', 'node_7_19': 'acc', 'node_7_20': 'acc', 'node_7_21': 'unacc', 'node_7_22': 'acc', 'node_7_23': 'vgood', 'node_7_24': 'vgood', 'node_7_25': 'vgood', 'node_7_26': 'acc', 'node_7_27': 'unacc', 'node_7_28': 'acc', 'node_7_29': 'good', 'node_7_30': 'vgood', 'node_7_31': 'vgood', 'node_7_32': 'unacc', 'node_7_33': 'good', 'node_7_34': 'good', 'node_7_35': 'good', 'node_7_36': 'vgood', 'node_7_37': 'good', 'node_7_38': 'vgood', 'node_7_39': 'good', 'node_7_40': 'acc', 'node_7_41': 'vgood', 'node_7_42': 'vgood', 'node_7_43': 'vgood', 'node_7_44': 'good', 'node_7_45': 'good', 'node_7_46': 'vgood', 'node_7_47': 'vgood', 'node_7_48': 'acc', 'node_7_49': 'acc', 'node_7_50': 'vgood', 'node_7_51': 'good', 'node_7_52': 'vgood', 'node_7_53': 'unacc', 'node_7_54': 'acc', 'node_7_55': 'acc', 'node_7_56': 'acc', 'node_7_57': 'unacc', 'node_7_58': 'acc', 'node_7_59': 'unacc', 'node_7_60': 'acc', 'node_7_61': 'unacc', 'node_7_62': 'acc', 'node_7_63': 'acc', 'node_7_64': 'good', 'node_7_65': 'good', 'node_7_66': 'acc', 'node_7_67': 'acc', 'node_7_68': 'acc', 'node_7_69': 'good', 'node_7_70': 'acc', 'node_7_71': 'acc', 'node_7_72': 'unacc', 'node_7_73': 'acc', 'node_7_74': 'acc', 'node_7_75': 'unacc', 'node_7_76': 'acc', 'node_7_77': 'acc', 'node_7_78': 'acc', 'node_7_79': 'acc', 'node_7_80': 'good', 'node_7_81': 'good', 'node_7_82': 'unacc', 'node_7_83': 'acc', 'node_7_84': 'acc', 'node_7_85': 'unacc', 'node_7_86': 'unacc', 'node_7_87': 'acc', 'node_7_88': 'acc', 'node_7_89': 'acc', 'node_7_90': 'unacc', 'node_7_91': 'acc', 'node_7_92': 'acc', 'node_7_93': 'acc', 'node_7_94': 'unacc', 'node_7_95': 'acc', 'node_7_96': 'acc', 'node_7_97': 'acc', 'node_7_98': 'good', 'node_7_99': 'acc', 'node_7_100': 'good', 'node_7_101': 'unacc', 'node_7_102': 'acc', 'node_7_103': 'acc', 'node_7_104': 'acc', 'node_7_105': 'unacc', 'node_7_106': 'unacc', 'node_7_107': 'acc', 'node_7_108': 'acc', 'node_7_109': 'acc', 'node_7_110': 'unacc', 'node_7_111': 'acc', 'node_7_112': 'unacc', 'node_7_113': 'acc', 'node_7_114': 'acc', 'node_7_115': 'acc', 'node_7_116': 'unacc', 'node_7_117': 'acc', 'node_7_118': 'acc'}\n","\n"," node types:\n","{'node_1_1': 'Internal', 'node_2_1': 'Internal', 'node_2_2': 'Leaf', 'node_2_3': 'Internal', 'node_3_1': 'Internal', 'node_3_2': 'Leaf', 'node_3_3': 'Internal', 'node_3_4': 'Internal', 'node_3_5': 'Internal', 'node_3_6': 'Leaf', 'node_4_1': 'Internal', 'node_4_2': 'Internal', 'node_4_3': 'Internal', 'node_4_4': 'Internal', 'node_4_5': 'Internal', 'node_4_6': 'Internal', 'node_4_7': 'Internal', 'node_4_8': 'Internal', 'node_4_9': 'Internal', 'node_4_10': 'Internal', 'node_4_11': 'Internal', 'node_4_12': 'Internal', 'node_4_13': 'Internal', 'node_4_14': 'Internal', 'node_4_15': 'Internal', 'node_4_16': 'Internal', 'node_5_1': 'Leaf', 'node_5_2': 'Leaf', 'node_5_3': 'Leaf', 'node_5_4': 'Internal', 'node_5_5': 'Internal', 'node_5_6': 'Internal', 'node_5_7': 'Internal', 'node_5_8': 'Internal', 'node_5_9': 'Leaf', 'node_5_10': 'Internal', 'node_5_11': 'Leaf', 'node_5_12': 'Leaf', 'node_5_13': 'Leaf', 'node_5_14': 'Internal', 'node_5_15': 'Internal', 'node_5_16': 'Internal', 'node_5_17': 'Leaf', 'node_5_18': 'Leaf', 'node_5_19': 'Leaf', 'node_5_20': 'Leaf', 'node_5_21': 'Internal', 'node_5_22': 'Internal', 'node_5_23': 'Internal', 'node_5_24': 'Leaf', 'node_5_25': 'Leaf', 'node_5_26': 'Leaf', 'node_5_27': 'Leaf', 'node_5_28': 'Leaf', 'node_5_29': 'Leaf', 'node_5_30': 'Internal', 'node_5_31': 'Internal', 'node_5_32': 'Leaf', 'node_5_33': 'Internal', 'node_5_34': 'Leaf', 'node_5_35': 'Internal', 'node_5_36': 'Leaf', 'node_5_37': 'Internal', 'node_5_38': 'Leaf', 'node_5_39': 'Internal', 'node_5_40': 'Internal', 'node_5_41': 'Leaf', 'node_5_42': 'Internal', 'node_5_43': 'Internal', 'node_5_44': 'Internal', 'node_5_45': 'Internal', 'node_5_46': 'Leaf', 'node_5_47': 'Internal', 'node_5_48': 'Internal', 'node_5_49': 'Leaf', 'node_5_50': 'Internal', 'node_5_51': 'Internal', 'node_5_52': 'Leaf', 'node_5_53': 'Internal', 'node_5_54': 'Internal', 'node_5_55': 'Internal', 'node_5_56': 'Internal', 'node_5_57': 'Leaf', 'node_5_58': 'Internal', 'node_5_59': 'Internal', 'node_5_60': 'Leaf', 'node_5_61': 'Leaf', 'node_5_62': 'Internal', 'node_6_1': 'Leaf', 'node_6_2': 'Internal', 'node_6_3': 'Leaf', 'node_6_4': 'Leaf', 'node_6_5': 'Leaf', 'node_6_6': 'Internal', 'node_6_7': 'Leaf', 'node_6_8': 'Internal', 'node_6_9': 'Leaf', 'node_6_10': 'Leaf', 'node_6_11': 'Internal', 'node_6_12': 'Leaf', 'node_6_13': 'Leaf', 'node_6_14': 'Leaf', 'node_6_15': 'Leaf', 'node_6_16': 'Internal', 'node_6_17': 'Leaf', 'node_6_18': 'Leaf', 'node_6_19': 'Internal', 'node_6_20': 'Leaf', 'node_6_21': 'Leaf', 'node_6_22': 'Internal', 'node_6_23': 'Internal', 'node_6_24': 'Leaf', 'node_6_25': 'Leaf', 'node_6_26': 'Leaf', 'node_6_27': 'Leaf', 'node_6_28': 'Internal', 'node_6_29': 'Internal', 'node_6_30': 'Leaf', 'node_6_31': 'Internal', 'node_6_32': 'Leaf', 'node_6_33': 'Leaf', 'node_6_34': 'Internal', 'node_6_35': 'Leaf', 'node_6_36': 'Leaf', 'node_6_37': 'Internal', 'node_6_38': 'Leaf', 'node_6_39': 'Leaf', 'node_6_40': 'Internal', 'node_6_41': 'Internal', 'node_6_42': 'Leaf', 'node_6_43': 'Leaf', 'node_6_44': 'Internal', 'node_6_45': 'Leaf', 'node_6_46': 'Leaf', 'node_6_47': 'Internal', 'node_6_48': 'Leaf', 'node_6_49': 'Leaf', 'node_6_50': 'Internal', 'node_6_51': 'Leaf', 'node_6_52': 'Leaf', 'node_6_53': 'Leaf', 'node_6_54': 'Internal', 'node_6_55': 'Leaf', 'node_6_56': 'Internal', 'node_6_57': 'Leaf', 'node_6_58': 'Leaf', 'node_6_59': 'Internal', 'node_6_60': 'Leaf', 'node_6_61': 'Leaf', 'node_6_62': 'Internal', 'node_6_63': 'Leaf', 'node_6_64': 'Leaf', 'node_6_65': 'Internal', 'node_6_66': 'Leaf', 'node_6_67': 'Leaf', 'node_6_68': 'Leaf', 'node_6_69': 'Leaf', 'node_6_70': 'Leaf', 'node_6_71': 'Internal', 'node_6_72': 'Leaf', 'node_6_73': 'Internal', 'node_6_74': 'Leaf', 'node_6_75': 'Leaf', 'node_6_76': 'Leaf', 'node_6_77': 'Leaf', 'node_6_78': 'Leaf', 'node_6_79': 'Internal', 'node_6_80': 'Internal', 'node_6_81': 'Leaf', 'node_6_82': 'Leaf', 'node_6_83': 'Leaf', 'node_6_84': 'Leaf', 'node_6_85': 'Leaf', 'node_6_86': 'Leaf', 'node_6_87': 'Internal', 'node_6_88': 'Leaf', 'node_6_89': 'Leaf', 'node_6_90': 'Leaf', 'node_6_91': 'Leaf', 'node_6_92': 'Leaf', 'node_6_93': 'Leaf', 'node_6_94': 'Internal', 'node_6_95': 'Internal', 'node_6_96': 'Leaf', 'node_6_97': 'Leaf', 'node_6_98': 'Internal', 'node_6_99': 'Leaf', 'node_6_100': 'Internal', 'node_6_101': 'Leaf', 'node_6_102': 'Leaf', 'node_6_103': 'Leaf', 'node_6_104': 'Internal', 'node_6_105': 'Leaf', 'node_6_106': 'Internal', 'node_6_107': 'Leaf', 'node_6_108': 'Leaf', 'node_6_109': 'Internal', 'node_6_110': 'Leaf', 'node_7_1': 'Leaf', 'node_7_2': 'Leaf', 'node_7_3': 'Leaf', 'node_7_4': 'Leaf', 'node_7_5': 'Leaf', 'node_7_6': 'Leaf', 'node_7_7': 'Leaf', 'node_7_8': 'Leaf', 'node_7_9': 'Leaf', 'node_7_10': 'Leaf', 'node_7_11': 'Leaf', 'node_7_12': 'Leaf', 'node_7_13': 'Leaf', 'node_7_14': 'Leaf', 'node_7_15': 'Leaf', 'node_7_16': 'Leaf', 'node_7_17': 'Leaf', 'node_7_18': 'Leaf', 'node_7_19': 'Leaf', 'node_7_20': 'Leaf', 'node_7_21': 'Leaf', 'node_7_22': 'Leaf', 'node_7_23': 'Leaf', 'node_7_24': 'Leaf', 'node_7_25': 'Leaf', 'node_7_26': 'Leaf', 'node_7_27': 'Leaf', 'node_7_28': 'Leaf', 'node_7_29': 'Leaf', 'node_7_30': 'Leaf', 'node_7_31': 'Leaf', 'node_7_32': 'Leaf', 'node_7_33': 'Leaf', 'node_7_34': 'Leaf', 'node_7_35': 'Leaf', 'node_7_36': 'Leaf', 'node_7_37': 'Leaf', 'node_7_38': 'Leaf', 'node_7_39': 'Leaf', 'node_7_40': 'Leaf', 'node_7_41': 'Leaf', 'node_7_42': 'Leaf', 'node_7_43': 'Leaf', 'node_7_44': 'Leaf', 'node_7_45': 'Leaf', 'node_7_46': 'Leaf', 'node_7_47': 'Leaf', 'node_7_48': 'Leaf', 'node_7_49': 'Leaf', 'node_7_50': 'Leaf', 'node_7_51': 'Leaf', 'node_7_52': 'Leaf', 'node_7_53': 'Leaf', 'node_7_54': 'Leaf', 'node_7_55': 'Leaf', 'node_7_56': 'Leaf', 'node_7_57': 'Leaf', 'node_7_58': 'Leaf', 'node_7_59': 'Leaf', 'node_7_60': 'Leaf', 'node_7_61': 'Leaf', 'node_7_62': 'Leaf', 'node_7_63': 'Leaf', 'node_7_64': 'Leaf', 'node_7_65': 'Leaf', 'node_7_66': 'Leaf', 'node_7_67': 'Leaf', 'node_7_68': 'Leaf', 'node_7_69': 'Leaf', 'node_7_70': 'Leaf', 'node_7_71': 'Leaf', 'node_7_72': 'Leaf', 'node_7_73': 'Leaf', 'node_7_74': 'Leaf', 'node_7_75': 'Leaf', 'node_7_76': 'Leaf', 'node_7_77': 'Leaf', 'node_7_78': 'Leaf', 'node_7_79': 'Leaf', 'node_7_80': 'Leaf', 'node_7_81': 'Leaf', 'node_7_82': 'Leaf', 'node_7_83': 'Leaf', 'node_7_84': 'Leaf', 'node_7_85': 'Leaf', 'node_7_86': 'Leaf', 'node_7_87': 'Leaf', 'node_7_88': 'Leaf', 'node_7_89': 'Leaf', 'node_7_90': 'Leaf', 'node_7_91': 'Leaf', 'node_7_92': 'Leaf', 'node_7_93': 'Leaf', 'node_7_94': 'Leaf', 'node_7_95': 'Leaf', 'node_7_96': 'Leaf', 'node_7_97': 'Leaf', 'node_7_98': 'Leaf', 'node_7_99': 'Leaf', 'node_7_100': 'Leaf', 'node_7_101': 'Leaf', 'node_7_102': 'Leaf', 'node_7_103': 'Leaf', 'node_7_104': 'Leaf', 'node_7_105': 'Leaf', 'node_7_106': 'Leaf', 'node_7_107': 'Leaf', 'node_7_108': 'Leaf', 'node_7_109': 'Leaf', 'node_7_110': 'Leaf', 'node_7_111': 'Leaf', 'node_7_112': 'Leaf', 'node_7_113': 'Leaf', 'node_7_114': 'Leaf', 'node_7_115': 'Leaf', 'node_7_116': 'Leaf', 'node_7_117': 'Leaf', 'node_7_118': 'Leaf'}\n"]}],"source":["# Check your implementation on training dataframe:\n","tree_model = tree_train(train_df, 0.9)\n","[tree_connectivity, node_labels, node_types] = tree_model\n","\n","print(\"\\n tree connectivity:\")\n","print(tree_connectivity)\n","\n","print(\"\\n node labels:\")\n","print(node_labels)\n","\n","print(\"\\n node types:\")\n","print(node_types)\n"]},{"cell_type":"markdown","metadata":{"id":"ty1X5YsJ25UT"},"source":["\n","\n","---\n","\n","# Part 7: Prediction by the Desicion Tree\n","(Q.7., **20 Marks**): Following the completion of decision tree training, the next step is to implement the prediction process through the trained tree structure. To achieve this, we need to create a function named 'tree_prediction.' This function takes two inputs: a test dataframe containing the samples to be predicted and the trained decision tree. It returns the predicted labels generated by the decision tree as a single DataFrame column."]},{"cell_type":"code","execution_count":81,"metadata":{"id":"0Yi1bTnb3rEE"},"outputs":[],"source":["def tree_prediction(testing_data, tree_model):\n","\n","  pred_labels = []\n","  # Unpack the tree_model list into three separate variables: tree_connectivity, node_labels, and node_types\n","  [tree_connectivity, node_labels, node_types] = tree_model\n","\n","  # Iterate through each sample in the testing_data\n","  for i in range(len(testing_data)):\n","  # for i in [1,2,3,4,5,6,7,8]:\n","    # Get a sample from the testing dataset\n","    sample = testing_data.loc[i]\n","\n","    # Start at the root node, which is always named \"node_1_1\"\n","    current_node = \"node_1_1\"\n","\n","  #.................................\n","  # write the rest here:\n","    # collect sample attributes\n","    sample_attr = [f\"{col}={val}\" for (col, val) in sample.items()]\n","\n","    # Begin a loop to traverse the decision tree until a leaf node is reached\n","    while node_types[current_node] == 'Internal':\n","      # walk tree_connectivity by using sample_attr to find next node\n","      connectivity = tree_connectivity[current_node]\n","      parent = current_node\n","      for attr in sample_attr:\n","        if attr in connectivity:\n","          current_node = connectivity[attr]\n","          break\n","\n","      # If all attr have been checked and current_node hasn't changed, then there\n","      # is an attribute that doesn't exist in our model (e.g., DOORS = 3).\n","      # In this case, break from the loop and just assume the label of the parent node\n","      if parent == current_node:\n","        break\n","\n","    # find the node label and put it in the pred_labels list\n","    pred_label = node_labels[current_node]\n","    pred_labels.append(pred_label)\n","  \n","  # convert pred_labels from list to Pandas Series\n","  pred_labels = pd.Series(pred_labels)\n","  #.................................\n","  # Return the Pandas Series containing the predicted labels\n","  return pred_labels"]},{"cell_type":"code","execution_count":82,"metadata":{"id":"gtIABZziDgSR"},"outputs":[{"name":"stdout","output_type":"stream","text":["0         acc\n","1       unacc\n","2       unacc\n","3       unacc\n","4       unacc\n","        ...  \n","1395    unacc\n","1396    unacc\n","1397    unacc\n","1398    unacc\n","1399    unacc\n","Length: 1400, dtype: object\n"]}],"source":["# Check your implementation on training dataframe:\n","tree_model = tree_train(train_df, 0.9)\n","pred_labels = tree_prediction(train_df, tree_model)\n","print(pred_labels)"]},{"cell_type":"markdown","metadata":{"id":"uRVHjnEi5lvn"},"source":["\n","\n","---\n","\n","## Part 8: Evaluating the Model\n","\n","* (Q.8-a, **5 Marks**)In the final step of this assignment, you'll apply the decision tree learning process. Start by training the decision tree on the training dataset using the 'tree_train' function, setting the terminating threshold to 0.9. Next, employ the 'tree_prediction' function, as previously implemented, to generate predictions for both the training and testing datasets. Following this, your task is to compare these predicted labels with the actual ground-truth labels to compute and report the accuracy rates for both the training and testing datasets.\n","\n","* (Q.8-b, **5 Marks**) Now, repeat the process with a different terminating threshold, specifically 0.7, and once again calculate and report the accuracy rates for the training and testing datasets. Finally, compare and contrast the results obtained with the two different threshold values (0.9 and 0.7). Provide an analysis and discussion of why one threshold might yield higher accuracy compared to the other."]},{"cell_type":"code","execution_count":83,"metadata":{"id":"D9Jt2EBp6uuU"},"outputs":[{"name":"stdout","output_type":"stream","text":["Evaluating the model at the 0.9 threshold\n","    Accuracy on the training dataset:  99.86%\n","    Accuracy on the testing dataset:  92.35%\n","Evaluating the model at the 0.7 threshold\n","    Accuracy on the training dataset:  71.21%\n","    Accuracy on the testing dataset:  65.14%\n"]}],"source":["#.................................\n","# write the rest here:\n","def accuracy(actual, expected):\n","    match_count = 0.0\n","    for i in range(len(expected)):\n","        match_count += 1 if actual.loc[i] == expected.loc[i] else 0\n","    return match_count / expected.count()\n","\n","datasets = {'training': train_df, 'testing': test_df}\n","\n","for threshold in [0.9,0.7]:\n","    print(f\"Evaluating the model at the {threshold} threshold\")\n","    for name, df in datasets.items():\n","        tree_model = tree_train(train_df, threshold)\n","        pred_labels = tree_prediction(df, tree_model)\n","\n","        # Select the last attribute of the DF as the actual labels\n","        class_attrs = df.columns[-1]\n","        actual_labels = df[class_attrs]\n","\n","        print(f\"    Accuracy on the {name} dataset: {accuracy(pred_labels, actual_labels) * 100: .2f}%\")\n","\n","\n","# Q.8-b Discussion:\n","# At both the 0.9 and 0.7 threshold levels, the accuracy of the model on the training dataset\n","# exceeds the accuracy of the model on the test data. This is because the test data contains\n","# some samples with attributes that do not exist in the training data the model was built from\n","# (e.g., \"DOORS = 3\").\n","#\n","# For both the training and test datasets, the accuracy of the model at the 0.9 threshold is\n","# higher than at the 0.7 threshold. This makes sense since the threshold value effectively\n","# determines the accuracy of the model. If the threshold is less than 1.0, then the model\n","# is expected to have less than perfect accuracy on the training data and hence even lower\n","# accuracy can be expected on the test data. If the threshold is reduced (e.g, 0.9 -> 0.7)\n","# then the accuracy will likewise be reduced. \n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMseztk+f92ev7YF44ZdQiz","provenance":[{"file_id":"19CmDcPf84La1mTvE0Gi0VE-ILrGARQt8","timestamp":1695758816173},{"file_id":"1_0_f28q3RsHEbCI9TXMfcytOE-A_8xU7","timestamp":1695591943950}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
